{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvh-dGR7Voqh"
      },
      "outputs": [],
      "source": [
        "! pip install spektral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "F4oFHnuuTxzE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "Bqrt5FG2pg-o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout, Input\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "from spektral.data.loaders import SingleLoader\n",
        "from spektral.datasets.citation import Citation\n",
        "from spektral.layers import GATConv\n",
        "from spektral.transforms import LayerPreprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "FPcCcHuSPUX5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "kU_fFwLW9W7k"
      },
      "outputs": [],
      "source": [
        "import spektral.data.graph as gg\n",
        "from scipy import sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generator"
      ],
      "metadata": {
        "id": "acpWcTUEm2hS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "z1sv3JvAxYtI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import itertools\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DictionaryLookupDataset(object):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.edges, self.empty_id = self.init_edges()\n",
        "      \n",
        "    def init_edges(self):\n",
        "        targets = range(0, self.size)\n",
        "        sources = range(self.size, self.size * 2)\n",
        "        next_unused_id = self.size\n",
        "        all_pairs = itertools.product(sources, targets)\n",
        "        edges = [list(i) for i in zip(*all_pairs)]\n",
        "\n",
        "        return edges, next_unused_id\n",
        "\n",
        "    def create_empty_graph(self, add_self_loops=False):\n",
        "      edge_index = np.array(self.edges, dtype=np.long)\n",
        "      return edge_index\n",
        "    \n",
        "    def get_combinations(self):\n",
        "      # returns: an iterable of [permutation(size)]\n",
        "      # number of combinations: size!\n",
        "      max_examples = 32000 # starting to affect from size=8, because 8!==40320\n",
        "      if math.factorial(self.size) > max_examples:\n",
        "        permutations = [np.random.permutation(range(self.size)) for _ in range(max_examples)]\n",
        "      else:\n",
        "        permutations = itertools.permutations(range(self.size))\n",
        "        \n",
        "      return permutations\n",
        "    \n",
        "    def generate_data(self, train_fraction, unseen_combs):\n",
        "      data_list = []\n",
        "      for perm in self.get_combinations():\n",
        "        edge_index = self.create_empty_graph(add_self_loops=False)\n",
        "        edge_index = sparse.csr_matrix((np.ones(self.size*self.size),(edge_index[0],edge_index[1])),shape=(self.size*2,self.size*2))\n",
        "        nodes = np.array(self.get_nodes_features(perm),dtype=np.long)\n",
        "        target_mask =  np.array([True] * (self.size) + [False] * self.size, dtype=np.bool)\n",
        "        labels = np.array(perm, dtype=np.long)\n",
        "   \n",
        "        \n",
        "        data_list.append(gg.Graph(x=nodes, a=edge_index, target_mask=target_mask, y=labels))\n",
        "\n",
        "\n",
        "      dim0, out_dim = self.get_dims()\n",
        "      if unseen_combs:\n",
        "        X_train, X_test = self.unseen_combs_train_test_split(data_list, train_fraction=train_fraction, shuffle=True)\n",
        "      else:\n",
        "        X_train, X_test = train_test_split(data_list, train_size=train_fraction, shuffle=True)\n",
        "\n",
        "      return X_train, X_test, dim0, out_dim\n",
        "\n",
        "    def get_nodes_features(self, perm):\n",
        "      # perm: a list of indices\n",
        "      #Node features is basically {[(A,_),(B,_),...(D,_)] , [(A,1),(B,2),...(D,4)]}.\n",
        "      #Then what is nodes,5,6,7,8,9? These are node numberings. there exists 2k=10 nodes and each have features i.e. a 2-tuple.\n",
        "      # The first row contains (key, empty_id)\n",
        "      # The second row contains (key, value) where the order of values is according to perm\n",
        "      nodes = [(key, self.empty_id) for key in range(self.size)]\n",
        "      for key, val in zip(range(self.size), perm):\n",
        "        nodes.append((key, val))\n",
        "\n",
        "      return nodes\n",
        "\n",
        "    def get_dims(self):\n",
        "      # get input and output dims\n",
        "      in_dim = self.size + 1\n",
        "      out_dim = self.size\n",
        "      return in_dim, out_dim\n",
        "\n",
        "    def unseen_combs_train_test_split(self, data_list, train_fraction, shuffle=True):\n",
        "      per_position_fraction = train_fraction ** (1 / self.size)\n",
        "      num_training_pairs = int(per_position_fraction * (self.size ** 2))\n",
        "      allowed_positions = set(random.sample(list(itertools.product(range(self.size), range(self.size))), num_training_pairs))\n",
        "      train = []\n",
        "      test = []\n",
        "        \n",
        "      for example in data_list:\n",
        "        if all([(i, label.item()) in allowed_positions for i, label in enumerate(example.y)]):\n",
        "          train.append(example)\n",
        "        else:\n",
        "          test.append(example)\n",
        "        \n",
        "        if shuffle:\n",
        "            random.shuffle(train)\n",
        "      return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {
        "id": "jsKteCF2vClK"
      },
      "outputs": [],
      "source": [
        "nodes_num = 5\n",
        "dictionary = DictionaryLookupDataset(nodes_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9-OM-D8_bDD"
      },
      "outputs": [],
      "source": [
        "dictionary.get_dims()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKx4ACay6THF"
      },
      "outputs": [],
      "source": [
        "if nodes_num == 5:\n",
        "  X_train, X_test, dim0, out_dim = dictionary.generate_data(0.75,False)\n",
        "if nodes_num == 4:\n",
        "  X_train, X_test, dim0, out_dim = dictionary.generate_data(0.85,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {
        "id": "JsSPNVIXTwiV"
      },
      "outputs": [],
      "source": [
        "from spektral.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return Dataset object."
      ],
      "metadata": {
        "id": "-fUwxMLHnYUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {
        "id": "Ct6T29_LZ9pK"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset of five random graphs.\n",
        "    \"\"\"\n",
        "    def __init__(self, list_g, **kwargs):\n",
        "        self.list_g = list_g\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def read(self):\n",
        "      return self.list_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "metadata": {
        "id": "2UtzMrdNRTN6"
      },
      "outputs": [],
      "source": [
        "md1 = MyDataset(X_train)\n",
        "md2 = MyDataset(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL:GAT"
      ],
      "metadata": {
        "id": "mp0Jhkq3nelE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "9tdQ1Z1O9H3b"
      },
      "outputs": [],
      "source": [
        "#GAT\n",
        "\n",
        "from tensorflow.keras import constraints, initializers, regularizers\n",
        "from spektral.layers import ops\n",
        "from spektral.layers.convolutional.conv import Conv\n",
        "from spektral.layers.ops import modes\n",
        "\n",
        "\n",
        "class GAT(Conv):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        attn_heads=1,\n",
        "        concat_heads=True,\n",
        "        dropout_rate=0.5,\n",
        "        return_attn_coef=True,\n",
        "        add_self_loops=True,\n",
        "        activation=None,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        attn_kernel_initializer=\"glorot_uniform\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        attn_kernel_regularizer=None,\n",
        "        activity_regularizer=None,\n",
        "        kernel_constraint=None,\n",
        "        bias_constraint=None,\n",
        "        attn_kernel_constraint=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            **kwargs\n",
        "        )\n",
        "        self.channels = channels\n",
        "        self.attn_heads = attn_heads\n",
        "        self.concat_heads = concat_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.return_attn_coef = return_attn_coef\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.attn_kernel_initializer = initializers.get(attn_kernel_initializer)\n",
        "        self.attn_kernel_regularizer = regularizers.get(attn_kernel_regularizer)\n",
        "        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n",
        "\n",
        "        if concat_heads:\n",
        "            self.output_dim = self.channels * self.attn_heads\n",
        "        else:\n",
        "            self.output_dim = self.channels\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        input_dim = input_shape[0][-1]\n",
        "       \n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"kernel\",\n",
        "            shape=[input_dim, self.attn_heads, self.channels],\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "        )\n",
        "        self.attn_kernel_self = self.add_weight(\n",
        "            name=\"attn_kernel_self\",\n",
        "            shape=[self.channels, self.attn_heads, 1],\n",
        "            initializer=self.attn_kernel_initializer,\n",
        "            regularizer=self.attn_kernel_regularizer,\n",
        "            constraint=self.attn_kernel_constraint,\n",
        "        )\n",
        "        self.attn_kernel_neighs = self.add_weight(\n",
        "            name=\"attn_kernel_neigh\",\n",
        "            shape=[self.channels, self.attn_heads, 1],\n",
        "            initializer=self.attn_kernel_initializer,\n",
        "            regularizer=self.attn_kernel_regularizer,\n",
        "            constraint=self.attn_kernel_constraint,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=[self.output_dim],\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name=\"bias\",\n",
        "            )\n",
        "\n",
        "        self.dropout = Dropout(self.dropout_rate, dtype=self.dtype)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        x, a = inputs\n",
        "\n",
        "        mode = ops.autodetect_mode(x, a)\n",
        "        if mode == modes.SINGLE and K.is_sparse(a):\n",
        "            output, attn_coef = self._call_single(x, a)\n",
        "        else:\n",
        "            if K.is_sparse(a):\n",
        "                a = tf.sparse.to_dense(a)\n",
        "            output, attn_coef = self._call_dense(x, a)\n",
        "\n",
        "        if self.concat_heads:\n",
        "            shape = tf.concat(\n",
        "                (tf.shape(output)[:-2], [self.attn_heads * self.channels]), axis=0\n",
        "            )\n",
        "            output = tf.reshape(output, shape)\n",
        "        else:\n",
        "            output = tf.reduce_mean(output, axis=-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "        if mask is not None:\n",
        "            output *= mask[0]\n",
        "        output = self.activation(output)\n",
        "\n",
        "        if self.return_attn_coef:\n",
        "            return output, attn_coef\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def _call_single(self, x, a):\n",
        "        # Reshape kernels for efficient message-passing\n",
        "        kernel = tf.reshape(self.kernel, (-1, self.attn_heads * self.channels))\n",
        "        attn_kernel_self = ops.transpose(self.attn_kernel_self, (2, 1, 0))\n",
        "        attn_kernel_neighs = ops.transpose(self.attn_kernel_neighs, (2, 1, 0))\n",
        "\n",
        "        \n",
        "        indices = a.indices\n",
        "\n",
        "        N = tf.shape(x, out_type=indices.dtype)[-2]\n",
        "  \n",
        "        if self.add_self_loops:\n",
        "            indices = ops.add_self_loops_indices(indices, N)\n",
        "       \n",
        "        targets, sources = indices[:, 1], indices[:, 0]\n",
        "\n",
        "        x = K.dot(x, kernel)\n",
        " \n",
        "        x = tf.reshape(x, (-1, self.attn_heads, self.channels))\n",
        "\n",
        "        # Compute attention\n",
        "        attn_for_self = tf.reduce_sum(x * attn_kernel_self, -1) #sums up \"deep\" hidden representation after attention kernel operation.\n",
        "        #garbage1 = tf.gather(x * attn_kernel_self, targets)\n",
        "\n",
        "        attn_for_self = tf.gather(attn_for_self, targets) #targets recieve attention hence attention for self. e(h_i,h_j) -> edge j to i so source=j and target=i\n",
        "\n",
        "        attn_for_neighs = tf.reduce_sum(x * attn_kernel_neighs, -1)\n",
        "        #garbage2 = tf.gather(x * attn_kernel_neighs, sources)\n",
        "        attn_for_neighs = tf.gather(attn_for_neighs, sources) #sources give attention.\n",
        "\n",
        "        attn_coef = attn_for_self + attn_for_neighs\n",
        "\n",
        "        attn_coef = tf.nn.leaky_relu(attn_coef, alpha=0.2)\n",
        "\n",
        "        attn_coef = ops.unsorted_segment_softmax(attn_coef, targets, N)\n",
        "\n",
        "        attn_coef = self.dropout(attn_coef)\n",
        "\n",
        "        attn_coef = attn_coef[..., None]\n",
        "\n",
        "        output = attn_coef * tf.gather(x, sources)\n",
        "\n",
        "        output = tf.math.unsorted_segment_sum(output, targets, N)\n",
        "        \n",
        "        return output, attn_coef\n",
        "        #(garbage1,garbage2)\n",
        "\n",
        "    def _call_dense(self, x, a):\n",
        "        shape = tf.shape(a)[:-1]\n",
        "        if self.add_self_loops:\n",
        "            a = tf.linalg.set_diag(a, tf.ones(shape, a.dtype))\n",
        "\n",
        "        x = tf.einsum(\"...NI , IHO -> ...NHO\", x, self.kernel)\n",
        " \n",
        "\n",
        "        attn_for_self = tf.einsum(\"...NHI , IHO -> ...NHO\", x, self.attn_kernel_self)\n",
        "\n",
        "\n",
        "        attn_for_neighs = tf.einsum(\n",
        "            \"...NHI , IHO -> ...NHO\", x, self.attn_kernel_neighs\n",
        "        )\n",
        "\n",
        "        attn_for_neighs = tf.einsum(\"...ABC -> ...CBA\", attn_for_neighs)\n",
        "\n",
        "\n",
        "        attn_coef = attn_for_self + attn_for_neighs\n",
        "        attn_coef = tf.nn.leaky_relu(attn_coef, alpha=0.2)\n",
        "\n",
        "\n",
        "        mask = tf.where(a == 0.0, -10e9, 0.0)\n",
        "        mask = tf.cast(mask, dtype=attn_coef.dtype)\n",
        "        attn_coef += mask[..., None, :]\n",
        "        attn_coef = tf.nn.softmax(attn_coef, axis=-1)\n",
        "        attn_coef_drop = self.dropout(attn_coef)\n",
        "\n",
        "        output = tf.einsum(\"...NHM , ...MHI -> ...NHI\", attn_coef_drop, x)\n",
        "\n",
        "        return output, attn_coef\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        return {\n",
        "            \"channels\": self.channels,\n",
        "            \"attn_heads\": self.attn_heads,\n",
        "            \"concat_heads\": self.concat_heads,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "            \"return_attn_coef\": self.return_attn_coef,\n",
        "            \"attn_kernel_initializer\": initializers.serialize(\n",
        "                self.attn_kernel_initializer\n",
        "            ),\n",
        "            \"attn_kernel_regularizer\": regularizers.serialize(\n",
        "                self.attn_kernel_regularizer\n",
        "            ),\n",
        "            \"attn_kernel_constraint\": constraints.serialize(\n",
        "                self.attn_kernel_constraint\n",
        "            ),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izCYhX71xmZ9"
      },
      "source": [
        "MODEL2 GATv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {
        "id": "JwRMae5JxkcG"
      },
      "outputs": [],
      "source": [
        "#Gatv2\n",
        "\n",
        "from tensorflow.keras import constraints, initializers, regularizers\n",
        "from spektral.layers import ops\n",
        "from spektral.layers.convolutional.conv import Conv\n",
        "from spektral.layers.ops import modes\n",
        "\n",
        "\n",
        "class GATConv2(Conv):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        attn_heads=1,\n",
        "        concat_heads=True,#True for 8 heads\n",
        "        dropout_rate=0.5,\n",
        "        return_attn_coef=True,\n",
        "        add_self_loops=True,\n",
        "        activation=None,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        attn_kernel_initializer=\"glorot_uniform\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        attn_kernel_regularizer=None,\n",
        "        activity_regularizer=None,\n",
        "        kernel_constraint=None,\n",
        "        bias_constraint=None,\n",
        "        attn_kernel_constraint=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            **kwargs\n",
        "        )\n",
        "        self.channels = channels\n",
        "        self.attn_heads = attn_heads\n",
        "        self.concat_heads = concat_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.return_attn_coef = return_attn_coef\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.attn_kernel_initializer = initializers.get(attn_kernel_initializer)\n",
        "        self.attn_kernel_regularizer = regularizers.get(attn_kernel_regularizer)\n",
        "        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n",
        "\n",
        "        if concat_heads:\n",
        "            self.output_dim = self.channels * self.attn_heads\n",
        "        else:\n",
        "            self.output_dim = self.channels\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        input_dim = input_shape[0][-1]\n",
        "       \n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"kernel\",\n",
        "            shape=[input_dim, self.attn_heads, self.channels],\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "        )\n",
        "        self.attn_kernel_self = self.add_weight(\n",
        "            name=\"attn_kernel_self\",\n",
        "            shape=[self.channels, self.attn_heads, 1],\n",
        "            initializer=self.attn_kernel_initializer,\n",
        "            regularizer=self.attn_kernel_regularizer,\n",
        "            constraint=self.attn_kernel_constraint,\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=[self.output_dim],\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name=\"bias\",\n",
        "            )\n",
        "\n",
        "        self.dropout = Dropout(self.dropout_rate, dtype=self.dtype)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        x, a = inputs\n",
        "\n",
        "        mode = ops.autodetect_mode(x, a)\n",
        "        if mode == modes.SINGLE and K.is_sparse(a):\n",
        "            output, attn_coef = self._call_single(x, a)\n",
        "        \n",
        "\n",
        "        if self.concat_heads:\n",
        "            shape = tf.concat(\n",
        "                (tf.shape(output)[:-2], [self.attn_heads * self.channels]), axis=0\n",
        "            )\n",
        "            output = tf.reshape(output, shape)\n",
        "        else:\n",
        "            output = tf.reduce_mean(output, axis=-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "        if mask is not None:\n",
        "            output *= mask[0]\n",
        "        output = self.activation(output)\n",
        "\n",
        "        if self.return_attn_coef:\n",
        "            return output, attn_coef\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def _call_single(self, x, a):\n",
        "        # Reshape kernels for efficient message-passing\n",
        "        kernel = tf.reshape(self.kernel, (-1, self.attn_heads * self.channels))\n",
        "        attn_kernel_self = ops.transpose(self.attn_kernel_self, (2, 1, 0))\n",
        "   \n",
        "       \n",
        "        indices = a.indices\n",
        "\n",
        "        N = tf.shape(x, out_type=indices.dtype)[-2]\n",
        "   \n",
        "        if self.add_self_loops:\n",
        "            indices = ops.add_self_loops_indices(indices, N)\n",
        "       \n",
        "        targets, sources = indices[:, 1], indices[:, 0]\n",
        "\n",
        "  \n",
        "\n",
        "        x = K.dot(x, kernel)\n",
        "        x = tf.reshape(x, (-1, self.attn_heads, self.channels))\n",
        "        xr = tf.nn.leaky_relu(x, alpha=0.2)\n",
        "        attn_for_self = tf.reduce_sum(xr * attn_kernel_self, -1)\n",
        "        attn_for_self = tf.gather(attn_for_self, targets)       \n",
        "        \n",
        "        attn_coef = ops.unsorted_segment_softmax(attn_for_self, targets, N)\n",
        "        attn_coef = self.dropout(attn_coef)\n",
        "        attn_coef = attn_coef[..., None]\n",
        "       \n",
        "        output = attn_coef * tf.gather(x, sources)\n",
        "\n",
        "        output = tf.math.unsorted_segment_sum(output, targets, N)\n",
        "        \n",
        "        return output,attn_coef\n",
        "\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        return {\n",
        "            \"channels\": self.channels,\n",
        "            \"attn_heads\": self.attn_heads,\n",
        "            \"concat_heads\": self.concat_heads,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "            \"return_attn_coef\": self.return_attn_coef,\n",
        "            \"attn_kernel_initializer\": initializers.serialize(\n",
        "                self.attn_kernel_initializer\n",
        "            ),\n",
        "            \"attn_kernel_regularizer\": regularizers.serialize(\n",
        "                self.attn_kernel_regularizer\n",
        "            ),\n",
        "            \"attn_kernel_constraint\": constraints.serialize(\n",
        "                self.attn_kernel_constraint\n",
        "            ),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {
        "id": "jNKkduhAgBKe"
      },
      "outputs": [],
      "source": [
        "N = nodes_num*2  # Number of nodes in the graph\n",
        "F = 2  # Original size of node features\n",
        "n_out = nodes_num  # Number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "metadata": {
        "id": "hZKu_6XG9o1G"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "channels = 128  # Number of channels in each head of the first GAT layer\n",
        "n_attn_heads = 1  # Number of attention heads in first GAT layer\n",
        "dropout = 0.0  # Dropout rate for the features and adjacency matrix\n",
        "l2_reg = 2.5e-4  # L2 regularization rate\n",
        "learning_rate = 0.001#5e-3  # Learning rate\n",
        "epochs = 20000  # Number of training epochs\n",
        "patience = 100  # Patience for early stopping\n",
        "\n",
        "# Model definition\n",
        "x_in = Input(shape=(F))\n",
        "a_in = Input((N), sparse=True)\n",
        "#x_in = Input(shape=(None,F))\n",
        "#a_in = Input((None,N), sparse=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {
        "id": "SE-K7iEA9WRl"
      },
      "outputs": [],
      "source": [
        "keys = tf.keras.layers.Embedding(nodes_num+1, 128)(x_in[:,0])\n",
        "values = tf.keras.layers.Embedding(nodes_num+1, 128)(x_in[:,1])\n",
        "attr = keys + values\n",
        "layer = tf.keras.layers.ReLU()\n",
        "attr = layer(attr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {
        "id": "sXmVzQniBHGq"
      },
      "outputs": [],
      "source": [
        "#GATConv2 for gatv2\n",
        "gc_1 = GATConv2(\n",
        "    channels,\n",
        "    attn_heads=n_attn_heads,\n",
        "    concat_heads=False, #True When >=2heads else False\n",
        "    dropout_rate=dropout,\n",
        "    activation=\"relu\",\n",
        ")([attr, a_in])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 410,
      "metadata": {
        "id": "maXNEyPBBKsj"
      },
      "outputs": [],
      "source": [
        "#GATConv2 for gatv2\n",
        "gc_2 = GATConv2(\n",
        "    n_out,\n",
        "    attn_heads=1,\n",
        "    concat_heads=False, #always False\n",
        "    dropout_rate=dropout,\n",
        "    activation=\"softmax\",\n",
        ")([gc_1[0], a_in])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 411,
      "metadata": {
        "id": "-6kWgP3nkON6"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=[x_in, a_in], outputs=gc_2)\n",
        "optimizer = Adam(learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkKoD_ygBtSt"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "M8aBHRdAyz9U"
      },
      "outputs": [],
      "source": [
        "l=[]\n",
        "for step, g in enumerate(md1):\n",
        "  loader_tr  = SingleLoader(MyDataset([g]))\n",
        "  l.append(loader_tr.__next__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "nM_RYIXYCtrL"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "# Instantiate an optimizer.\n",
        "\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if nodes_num == 5:\n",
        "  div = 30\n",
        "  tot = 90\n",
        "if nodes_num == 4:\n",
        "  div = 5\n",
        "  tot = 20"
      ],
      "metadata": {
        "id": "KfJ7JK1nqWoT"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN"
      ],
      "metadata": {
        "id": "LVnGakn0qogH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCUo5-sLSpVx"
      },
      "outputs": [],
      "source": [
        "epochs = 4000\n",
        "acc=0\n",
        "running_loss = 0\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, g in enumerate(md1):\n",
        "\n",
        "        inputs, target = l[step]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            logits,_ = model(inputs, training=True)  # Logits for this minibatch\n",
        "\n",
        "            logits = logits[0:n_out]\n",
        "\n",
        "            loss_value = loss_fn(target, logits)\n",
        "\n",
        "            running_loss+=loss_value\n",
        "\n",
        "            acc += (tf.argmax(logits,1)==tf.reshape(target,-1)).numpy().sum()==len(tf.reshape(target,-1))\n",
        "\n",
        "        \n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "        if step%div==0:\n",
        "          old = [0]*len(grads)\n",
        "          for i,j in enumerate(grads):\n",
        "            if i >1:\n",
        "              old[i] = j + old[i]\n",
        "            else:\n",
        "              old[i] = tf.IndexedSlices(j.values + old[i] , j.indices,  j.dense_shape) \n",
        "      \n",
        "        else:\n",
        "          for i,j in enumerate(grads):\n",
        "            if i >1:\n",
        "              old[i] = j + old[i] \n",
        "            else:\n",
        "              old[i] = tf.IndexedSlices(j.values + old[i].values , j.indices,  j.dense_shape) \n",
        "        \n",
        "\n",
        "        if step % div == div-1:\n",
        "    \n",
        "          for i,j in enumerate(old):\n",
        "            if i >1:\n",
        "              old[i] = j/div\n",
        "            else:\n",
        "              old[i] = tf.IndexedSlices(j.values/div , j.indices,  j.dense_shape) \n",
        "            \n",
        "          optimizer.apply_gradients(zip(old, model.trainable_weights))\n",
        "          old = [0]*len(grads)\n",
        "\n",
        "        if step == 0 and epoch!=0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(running_loss/tot))\n",
        "            )\n",
        "            \n",
        "            print(\n",
        "                \"Accuracy (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(acc/tot))\n",
        "            )\n",
        "            acc=0\n",
        "            running_loss=0\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "0PNZ-Zi4rGxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {
        "id": "qEaXqi8DQkOK"
      },
      "outputs": [],
      "source": [
        "l=[]\n",
        "for step, g in enumerate(md2):\n",
        "  loader_tr  = SingleLoader(MyDataset([g]))\n",
        "  l.append(loader_tr.__next__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca7ATdUkQUcm"
      },
      "outputs": [],
      "source": [
        "acc=0\n",
        "running_loss = 0\n",
        "\n",
        "for step, g in enumerate(md2):\n",
        "\n",
        "    inputs, target = l[step]\n",
        "\n",
        "    logits,_ = model(inputs, training=False)  # Logits for this minibatch\n",
        "\n",
        "    logits = logits[0:n_out]\n",
        "\n",
        "    loss_value = loss_fn(target, logits)\n",
        "    running_loss+=loss_value\n",
        "    acc += (tf.argmax(logits,1)==tf.reshape(target,-1)).numpy().sum()==len(tf.reshape(target,-1))\n",
        "\n",
        "print(\"f-loss\",running_loss/len(md2))\n",
        "print(\"f-acc\",acc/len(md2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load google drive to load model checkpoints."
      ],
      "metadata": {
        "id": "_qD5TNRorMlc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ7x7DAdURFD",
        "outputId": "08b80281-af4a-4ae7-c001-7ee5ef063ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o2k4KVaV0SS"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x5g5CCE94_b"
      },
      "outputs": [],
      "source": [
        "%cd checkpoints/gat2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "VpABFX6ZWJ4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3ca169-4812-48c0-a2bc-27e4247c639b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gat-8heads-k4.data-00000-of-00001      GATv2-1heads-k5-2.index\n",
            "gat-8heads-k4.index\t\t       GATv2-8heads-k4-1.data-00000-of-00001\n",
            "GAT-8heads-k5.data-00000-of-00001      GATv2-8heads-k4-1.index\n",
            "GAT-8heads-k5.index\t\t       GATv2-8heads-k5.data-00000-of-00001\n",
            "GATv2-1heads-k4-2.data-00000-of-00001  GATv2-8heads-k5.index\n",
            "GATv2-1heads-k4-2.index\t\t       real-GAT-1heads-k4.data-00000-of-00001\n",
            "GATv2-1heads-k5-2.data-00000-of-00001  real-GAT-1heads-k4.index\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZs9T9HkUY1I"
      },
      "outputs": [],
      "source": [
        "model.load_weights('GATv2-1heads-k5-2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y2Au9d7Afxn"
      },
      "source": [
        "'GATv2-1heads-k5-2'<br>\n",
        "'GATv2-1heads-k4-2'<br>\n",
        "GATv2-8heads-k4-1'<br>\n",
        "'real-GAT-1heads-k4'<br>\n",
        "'gat-8heads-k4'<br>\n",
        "'GATv2-8heads-k5'<br>\n",
        "'GAT-8heads-k5'<br>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GAT-version2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}